{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b82695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.10.0 torchvision==0.11.1 torchtext==0.11.0 torchaudio==0.10.0 --quiet\n",
    "\n",
    "#!pip install pytorch-lightning==1.5.2 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97b87ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.10.0+cu102\n",
      "pytorch ligthening version: 1.5.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"torch version:\",torch.__version__)\n",
    "\n",
    "print(\"pytorch ligthening version:\",pl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0a69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a truth table that matches the XOR gate. We will use the variables to create the dataset\n",
    "xor_inputs = [Variable(torch.Tensor([0, 0])),\n",
    "\n",
    "          Variable(torch.Tensor([0, 1])),\n",
    "\n",
    "          Variable(torch.Tensor([1, 0])),\n",
    "\n",
    "          Variable(torch.Tensor([1, 1]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae82c8",
   "metadata": {},
   "source": [
    "in the code above, we created four tensors, and each tensor had two values—that is, it had two features, A and B. We are ready with all the input features. A total of four rows are ready to be fed to our XOR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7352ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the input features are ready, it's time to build our target variables\n",
    "xor_targets = [Variable(torch.Tensor([0])),\n",
    "\n",
    "           Variable(torch.Tensor([1])),\n",
    "\n",
    "           Variable(torch.Tensor([1])),\n",
    "\n",
    "           Variable(torch.Tensor([0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd8213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs and targets will be ready in the final step of preparing our dataset. It's time to create a data loader\n",
    "xor_data = list(zip(xor_inputs, xor_targets)) # creating a dataset that is a list of tuples, and each tuple has two values- \n",
    "#first values are the two features/inputs, and the second values are the target values for the given input\n",
    "\n",
    "train_loader = DataLoader(xor_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec9c67",
   "metadata": {},
   "source": [
    "Data loaders in PyTorch Lightning look for two main things—the key and the value, which in our case are the features and target values. We are then using the DataLoader module from torch.utils.data to wrap the xor_data and create a Python iterable over the XOR data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7773ff8c",
   "metadata": {},
   "source": [
    "Every model we build using PyTorch Lightning must be inherited from a class called LightningModule.\n",
    "In simple terms, we can say that PyTorch LightningModule is the same as PyTorch nn.Module but with added life cycle methods and other operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a61de0",
   "metadata": {},
   "source": [
    "Any PyTorch Lightning model needs at least two life cycle methods—one for the training loop to train the model (called training_step), and another to configure an optimizer for the model (called configure_optimizers). In addition to these two life cycle methods, we also use the forward method. This is where we take in the input data and pass it to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6862f",
   "metadata": {},
   "source": [
    "Our XOR MLP model building follows this process, and we will go over each step in detail, as follows:  \n",
    "\n",
    " - Initializing the model  \n",
    " - Mapping inputs to the model  \n",
    " - Configuring the optimizer  \n",
    " - Setting up training parameters  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7633c0",
   "metadata": {},
   "source": [
    "# Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f16e1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class XORModel(pl.LightningModule):  #creating a class called XOR that inherits from PyTorch LightningModule\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        super(XORModel,self).__init__()\n",
    "\n",
    "\n",
    "        #set up the hidden layers\n",
    "        self.input_layer = nn.Linear(2, 4) #1st layer - 2 inputs and returns 4 outputs\n",
    "        self.output_layer = nn.Linear(4,1)\n",
    "\n",
    "        #Initialize the activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        #Initialize the loss function\n",
    "        self.loss = nn.MSELoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585da9d",
   "metadata": {},
   "source": [
    "# Mapping inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e882e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward method, which takes the inputs and generates the model's output\n",
    "# The forward method acts as a mapper or medium where data is passed between multiple layers and the activation function\n",
    "\n",
    "def forward(self, input):\n",
    "\n",
    "    #print(\"INPUT:\", input.shape)\n",
    "\n",
    "    x = self.input_layer(input)\n",
    "\n",
    "    #print(\"FIRST:\", x.shape)\n",
    "\n",
    "    x = self.sigmoid(x)\n",
    "\n",
    "    #print(\"SECOND:\", x.shape)\n",
    "\n",
    "    output = self.output_layer(x)\n",
    "\n",
    "    #print(\"THIRD:\", output.shape)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b7c5f",
   "metadata": {},
   "source": [
    "# Configuring the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bbe4d",
   "metadata": {},
   "source": [
    "All optimizers in PyTorch Lightning can be configured in a life cycle method called **configure_optimizers**. In this method, one or multiple optimizers can be configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfa3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self):\n",
    "\n",
    "    params = self.parameters() #model parameters can be accessed by using the self object with the self.parameters() method\n",
    "\n",
    "    optimizer = optim.Adam(params=params, lr = 0.01)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09366136",
   "metadata": {},
   "source": [
    "# Setting up training parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9b672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where all the model training occurs\n",
    "def training_step(self, batch, batch_idx): \n",
    "    '''\n",
    "    batch: Data that is being passed in the data loader is accessed in batches. \n",
    "           This consists of two items: one is the input/features data, and the other item is targets.\n",
    "    \n",
    "    batch_idx: This is the index number or the sequence number for the batche of data.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    xor_input, xor_target = batch\n",
    "\n",
    "    #print(\"XOR INPUT:\", xor_input.shape)\n",
    "\n",
    "    #print(\"XOR TARGET:\", xor_target.shape)\n",
    "\n",
    "    outputs = self(xor_input)\n",
    "\n",
    "    #print(\"XOR OUTPUT:\", outputs.shape)\n",
    "\n",
    "    loss = self.loss(outputs, xor_target)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d78a5",
   "metadata": {},
   "source": [
    "In the preceding method, we are accessing our inputs and targets from the batch and then passing the inputs to the self method. When the input is passed to the self method, that indirectly invokes our forward method, which returns the XOR multilayer NN output. We are using the MSE loss function to calculate the loss and return the loss value for this method.\n",
    "\n",
    "In short, inputs and targets passed to batch --> self method --> invokes forward method --> mapping of Input-output b/w layers --> finally loss value as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3e6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(XORModel,self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(2, 4)\n",
    "        self.output_layer = nn.Linear(4,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    # input to output mapping\n",
    "    def forward(self, input):\n",
    "\n",
    "        #print(\"INPUT:\", input.shape)\n",
    "        x = self.input_layer(input)\n",
    "        #print(\"FIRST:\", x.shape)\n",
    "\n",
    "        x = self.sigmoid(x)\n",
    "        #print(\"SECOND:\", x.shape)\n",
    "        output = self.output_layer(x)\n",
    "\n",
    "        #print(\"THIRD:\", output.shape)\n",
    "        return output\n",
    "    \n",
    "    # set up optimizer\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        params = self.parameters()\n",
    "        optimizer = optim.Adam(params=params, lr = 0.01)\n",
    "        return optimizer\n",
    "\n",
    "    # model is trained and you get loss value\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        xor_input, xor_target = batch\n",
    "\n",
    "        #print(\"XOR INPUT:\", xor_input.shape)\n",
    "        #print(\"XOR TARGET:\", xor_target.shape)\n",
    "        outputs = self(xor_input)\n",
    "\n",
    "        #print(\"XOR OUTPUT:\", outputs.shape)\n",
    "        loss = self.loss(outputs, xor_target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e14670",
   "metadata": {},
   "source": [
    "We are using sigmoid as our activation function, MSE as our loss function, and Adam as our optimizer  \n",
    "Backpropagation, clearing gradients, or optimizer parameter updates and many other things are taken care of by the PyTorch Lightning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52885d7",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2da23e",
   "metadata": {},
   "source": [
    "All models built in PyTorch Lightning can be trained using a Trainer class.  \n",
    "Everything is taken care inside trainer class - looping over the dataset, backpropagation, clearing gradients, \n",
    "and the optimizer step.  \n",
    "Also, the Trainer class supports many other functionalities that help us to build our model easily, and \n",
    "some of those functionalities are various callbacks, model checkpoints, early stopping, dev runs for unit testing,\n",
    "support for GPUs and TPUs, loggers, logs, epochs, and many more. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb01390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint()\n",
    "model = XORModel()\n",
    "\n",
    "#creating a trainer object for 100 epochs \n",
    "trainer = pl.Trainer(max_epochs=100, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38182537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 18:44:43.159320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-15 18:44:43.159352: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n",
      "  | Name         | Type    | Params\n",
      "-----------------------------------------\n",
      "0 | input_layer  | Linear  | 12    \n",
      "1 | output_layer | Linear  | 5     \n",
      "2 | sigmoid      | Sigmoid | 0     \n",
      "3 | loss         | MSELoss | 0     \n",
      "-----------------------------------------\n",
      "17        Trainable params\n",
      "0         Non-trainable params\n",
      "17        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/home/ju9yt3r/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ju9yt3r/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:406: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426b097578d141fd85d84222216d836e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae981c",
   "metadata": {},
   "source": [
    "In **PyTorch Lightning, one advantage** we see is whenever we train a model multiple times, **all the different model versions are saved to disk in a default folder called lightning_logs,** and once all the models with different versions are made ready, we always have the opportunity to load the different model versions from the files and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5760328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mversion_0\u001b[0m/  \u001b[01;34mversion_1\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a12617",
   "metadata": {},
   "source": [
    "Within these version subfolders, we have all the information about the model being trained and built, which can be easily loaded, and predictions can be performed. Files within these folders have some useful information, such as hyperparameters, which are saved as hparams.yaml, and we also have a subfolder called checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "795d487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightning_logs/version_0/:\r\n",
      "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/                                           hparams.yaml\r\n",
      "events.out.tfevents.1684167297.ip-10-215-9-65.23862.0\r\n",
      "\r\n",
      "lightning_logs/version_1/:\r\n",
      "\u001b[01;34mcheckpoints\u001b[0m/                                           hparams.yaml\r\n",
      "events.out.tfevents.1684176284.ip-10-215-9-65.25799.0\r\n"
     ]
    }
   ],
   "source": [
    "ls lightning_logs/*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ec602",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e30090b",
   "metadata": {},
   "source": [
    "identifying the latest version of a model can be done using checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1356d0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ju9yt3r/FOLDER_PYTORCH/TEST/lightning_logs/version_1/checkpoints/epoch=99-step=399.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_callback.best_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b423b98",
   "metadata": {},
   "source": [
    "Loading the model from the checkpoint can easily be done using the load_from_checkpoint method from the model object by passing the model checkpoint path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de694f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = model.load_from_checkpoint(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11547c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] 1\n",
      "[0, 1] 0\n",
      "[1, 0] 1\n",
      "[1, 1] 0\n"
     ]
    }
   ],
   "source": [
    "test = torch.utils.data.DataLoader(xor_inputs, batch_size=1)\n",
    "\n",
    "for val in xor_inputs:\n",
    "    _ = train_model(val)\n",
    "    print([int(val[0]),int(val[1])], int(_.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8810a193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0.]), tensor([0., 1.]), tensor([1., 0.]), tensor([1., 1.])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbc6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
