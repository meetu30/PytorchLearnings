{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87f6a4f",
   "metadata": {},
   "source": [
    "### building a CNN model for the Histopathologic Cancer Detection dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8338a6",
   "metadata": {},
   "source": [
    "The original image is cropped to 32x32 size. We then do the following:\n",
    "\n",
    " 1. We put it through the first convolution with a kernel size of 3 and a stride length of 1.\n",
    " 2. The first convolution layer is followed by the MaxPool layer, where images are converted to lower-dimensional 16x16 objects.\n",
    " 3. This is followed by another convolutional layer with six channels. This convolutional layer is followed by five fully connected layers of feature-length 1,000,500, 250, 120, and 60 to finally get a SoftMax layer that gives the final predictions.  \n",
    " \n",
    " We will be using **ReLU** as our activation function, **Adam** as our optimizer, and **Cross-Entropy** as our loss function for training this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341c4ed",
   "metadata": {},
   "source": [
    "PyTorch Lightning is a versatile framework, which makes training and scaling DL models easy by focusing on building models than writing complex programs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5dfd6d",
   "metadata": {},
   "source": [
    "Here are the steps for building an image classifier using a CNN:\n",
    "\n",
    "1. Importing the packages\n",
    "2. Collecting the data\n",
    "3. Preparing the data\n",
    "4. Building the model\n",
    "5. Training the model\n",
    "6. Evaluating the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d42069",
   "metadata": {},
   "source": [
    "### Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d94f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.10.0 torchvision==0.11.1 torchtext==0.11.0 torchaudio==0.10.0 --quiet\n",
    "\n",
    "!pip install pytorch-lightning==1.5.2 --quiet\n",
    "\n",
    "!pip install opendatasets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279c7e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "import opendatasets as od\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e53b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.4.2\n",
      "numpy version: 1.21.6\n",
      "torch version: 1.10.0+cu102\n",
      "pytorch ligthening version: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "print(\"pandas version:\",pd.__version__)\n",
    "\n",
    "print(\"numpy version:\",np.__version__)\n",
    "\n",
    "#print(\"seaborn version:\",sns.__version__)\n",
    "\n",
    "print(\"torch version:\",torch.__version__)\n",
    "\n",
    "print(\"pytorch ligthening version:\",pl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202527f6",
   "metadata": {},
   "source": [
    "### Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344dc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_url = 'https://www.kaggle.com/c/histopathologic-cancer-detection'\n",
    "\n",
    "#od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73afd2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f38a6374c348f90b587e046aac6079959adf3835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>755db6279dae599ebb4d39a9123cce439965282d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>068aba587a4950175d04c680d38943fd488d6a9d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  label\n",
       "0  f38a6374c348f90b587e046aac6079959adf3835      0\n",
       "1  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n",
       "2  755db6279dae599ebb4d39a9123cce439965282d      0\n",
       "3  bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n",
       "4  068aba587a4950175d04c680d38943fd488d6a9d      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data has 220,000 images and test data has 57,000 images\n",
    "cancer_labels = pd.read_csv(\"histopathologic-cancer-detection/train_labels.csv\")\n",
    "cancer_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "592faacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1152 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_count = cancer_labels.label.value_counts()\n",
    "\n",
    "plt.pie(labels_count, labels=['No Cancer', 'Cancer'], startangle=180, autopct='%1.1f')\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd90c09",
   "metadata": {},
   "source": [
    "### Preparing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4edc1",
   "metadata": {},
   "source": [
    "It involves the following steps:  \n",
    " - Downsampling the data\n",
    " - Loading the dataset\n",
    " - Augmenting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6ee901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downsample the 220,000 images in the train folder to 10,000 images and then split them into training and testing datasets\n",
    "np.random.seed(0)\n",
    "\n",
    "train_imgs_orig = os.listdir(\"histopathologic-cancer-detection/train\")\n",
    "\n",
    "selected_image_list = []\n",
    "\n",
    "for img in np.random.choice(train_imgs_orig, 10000):\n",
    "\n",
    "    selected_image_list.append(img)\n",
    "\n",
    "len(selected_image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ea0688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the downsampled training dataset:  8000\n",
      "Number of images in the downsampled testing dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "#train-test split\n",
    "np.random.seed(0)\n",
    "\n",
    "np.random.shuffle(selected_image_list)\n",
    "\n",
    "cancer_train_idx = selected_image_list[:8000]\n",
    "\n",
    "cancer_test_idx = selected_image_list[8000:]\n",
    "\n",
    "print(\"Number of images in the downsampled training dataset: \", len(cancer_train_idx))\n",
    "\n",
    "print(\"Number of images in the downsampled testing dataset: \", len(cancer_test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11653145",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in cancer_train_idx:\n",
    "\n",
    "    src = os.path.join('histopathologic-cancer-detection/train', fname)\n",
    "\n",
    "    dst = os.path.join('histopathologic-cancer-detection/train_dataset/', fname)\n",
    "\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8bf1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in cancer_test_idx:\n",
    "\n",
    "    src = os.path.join('histopathologic-cancer-detection/train', fname)\n",
    "\n",
    "    dst = os.path.join('histopathologic-cancer-detection/test_dataset/', fname)\n",
    "\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0474fa",
   "metadata": {},
   "source": [
    "The labels for the images that were selected in the downsampled data will be extracted in a list that will be used for training and evaluating the image classification model, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67595f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_image_labels = pd.DataFrame()\n",
    "\n",
    "id_list = []\n",
    "\n",
    "label_list = []\n",
    "\n",
    "for img in selected_image_list:\n",
    "\n",
    "    label_tuple = cancer_labels.loc[cancer_labels['id'] == img.split('.')[0]]\n",
    "\n",
    "    id_list.append(label_tuple['id'].values[0])\n",
    "\n",
    "    label_list.append(label_tuple['label'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0ca2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f256057c24a792973987c8a9145432ede22f44a1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4706bd0d1022fd71d764d25a31653db01828277b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1d67dda911af807586000a7ea39dc4bb781fdfe9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f020f2b5f831173c4ffb92b637feae8878fba9ac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55d1b423da9ff472eb23c3be7592501fa90a48ad</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  label\n",
       "0  f256057c24a792973987c8a9145432ede22f44a1      1\n",
       "1  4706bd0d1022fd71d764d25a31653db01828277b      1\n",
       "2  1d67dda911af807586000a7ea39dc4bb781fdfe9      0\n",
       "3  f020f2b5f831173c4ffb92b637feae8878fba9ac      1\n",
       "4  55d1b423da9ff472eb23c3be7592501fa90a48ad      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_image_labels['id'] = id_list\n",
    "selected_image_labels['label'] = label_list\n",
    "selected_image_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb5f22fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4f48a",
   "metadata": {},
   "source": [
    "The custom class defined below inherits from the **torch.utils.data.Dataset** module. The LoadCancerDataset custom class is initialized in the __init__ method and accepts three arguments:   \n",
    " - the path to the data folder, \n",
    " - the transformer with a default value of cropping the image to size 32 and transforming it to a tensor, and \n",
    " - a dictionary with the labels and IDs of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c502213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch Lightning expects data to be in folders with the classes. \n",
    "So, we cannot use the DataLoader module directly when all train/test images are in one folder without subfolders. \n",
    "Therefore, we will write our custom class for loading the data, as follows:\n",
    "\n",
    "'''\n",
    "class LoadCancerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_folder,    #path to data folder\n",
    "                 transform = T.Compose([T.CenterCrop(32),T.ToTensor()]), # transformer with a default value of cropping the image to size 32 and transforming it to a tensor\n",
    "                 dict_labels={} # dict with  labels and IDs of the dataset\n",
    "                ):\n",
    "\n",
    "        self.data_folder = data_folder\n",
    "\n",
    "        self.list_image_files = [s for s in os.listdir(data_folder)]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.dict_labels = dict_labels\n",
    "\n",
    "        self.labels = [dict_labels[i.split('.')[0]] for i in self.list_image_files]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.list_image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = os.path.join(self.data_folder, self.list_image_files[idx])\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        img_name_short = self.list_image_files[idx].split('.')[0]\n",
    "        label = self.dict_labels[img_name_short]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17d76e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_image_labels has all selcted 10,000 images, which are split into test and train dataset\n",
    "# here we are creating a dictionary to map ID and label \n",
    "img_class_dict = {k:v for k, v in zip(selected_image_labels.id, selected_image_labels.label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b015c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_T_train = T.Compose([\n",
    "\n",
    "    T.CenterCrop(32),\n",
    "\n",
    "    T.RandomHorizontalFlip(),\n",
    "\n",
    "    T.RandomVerticalFlip(),\n",
    "\n",
    "    T.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "data_T_test = T.Compose([\n",
    "\n",
    "    T.CenterCrop(32),\n",
    "\n",
    "    T.ToTensor(),\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759255b",
   "metadata": {},
   "source": [
    "we will call our LoadCancerDataset custom class with the path to the data folder, transformer, and the image label dictionary to convert it to the format accepted by the torch.utils.data.DataLoader module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10ba491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LoadCancerDataset(  data_folder='histopathologic-cancer-detection/train_dataset/',\n",
    "                                transform=data_T_train, \n",
    "                                dict_labels=img_class_dict  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f56df4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = LoadCancerDataset(  data_folder='histopathologic-cancer-detection/test_dataset/',\n",
    "                                transform=data_T_test, \n",
    "                                dict_labels=img_class_dict  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2d4d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size, num_workers=2, pin_memory=True, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c76cb",
   "metadata": {},
   "source": [
    "### Building the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dc6b4",
   "metadata": {},
   "source": [
    "Initializing the model  \n",
    "Configuring the optimizer  \n",
    "Configuring training and testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4aa5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNImageClassifier(pl.LightningModule):\n",
    "    def __init__(self, learning_rate = 0.001): # the CNN ImageClassifier class accepts a single parameter—the learning rate, \n",
    "                                               # with a default value of 0.001\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        #Input size (256, 3, 32, 32)\n",
    "\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3,out_channels=3,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        #output_shape: (256, 3, 32, 32)\n",
    "\n",
    "        self.relu1=nn.ReLU()\n",
    "\n",
    "        #output_shape: (256, 3, 32, 32)\n",
    "\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        #output_shape: (256, 3, 16, 16)\n",
    "\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        #output_shape: (256, 3, 16, 16)\n",
    "\n",
    "        self.relu2=nn.ReLU()\n",
    "\n",
    "        #output_shape: (256, 6, 16, 16)\n",
    "\n",
    "        #six fully connected linear layers followed by a loss function\n",
    "        self.fully_connected_1 =nn.Linear(in_features=16 * 16 * 6,out_features=1000) #takes the input, which is the output generated from conv_layer2, \n",
    "                                                                                    # and this self.fully_connected_1 layer outputs 1,000 nodes.\n",
    "\n",
    "        self.fully_connected_2 =nn.Linear(in_features=1000,out_features=500) #takes the output from the first linear layer and outputs 500 nodes\n",
    "\n",
    "        self.fully_connected_3 =nn.Linear(in_features=500,out_features=250) # takes the output from the 2nd linear layer and outputs 250 nodes\n",
    "\n",
    "        self.fully_connected_4 =nn.Linear(in_features=250,out_features=120) #takes the output from the 3rd linear layer and outputs 120 nodes\n",
    "\n",
    "        self.fully_connected_5 =nn.Linear(in_features=120,out_features=60) # takes the output from the 4th linear layer and outputs 60 nodes\n",
    "\n",
    "        self.fully_connected_6 =nn.Linear(in_features=60,out_features=2) # takes the output from the 5th linear layer and outputs 2 nodes\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    # Since this is a binary classification, the output of this NN architecture should be the probability for the two classes. \n",
    "    # Finally, we will initialize the loss function, which is cross-entropy loss.\n",
    "\n",
    "    '''\n",
    "    Our NN architecture is defined as a combination of CNN and fully connected linear networks, \n",
    "    so it's time to pass in the data from the different layers and the activation functions. \n",
    "    We will do this with the help of the forward method.\n",
    "    \n",
    "    To summarize, in the forward method, the input image data is first passed over the two convolution layers,\n",
    "    and then the output from the convolution layers is passed over six fully connected layers. \n",
    "    Finally, the output is returned.\n",
    "    \n",
    "    '''\n",
    "    def forward(self, input):\n",
    "\n",
    "        output=self.conv_layer1(input)\n",
    "\n",
    "        output=self.relu1(output)    \n",
    "\n",
    "        output=self.pool(output)\n",
    "\n",
    "        output=self.conv_layer2(output)\n",
    "\n",
    "        output=self.relu2(output)\n",
    "\n",
    "\n",
    "        # To pass the output to our linear layers, it is converted to a single-dimensional form, \n",
    "        # which can be achieved using the tensor view method\n",
    "        output=output.view(-1, 6*16*16)\n",
    "\n",
    "        output = self.fully_connected_1(output)\n",
    "\n",
    "        output = self.fully_connected_2(output)\n",
    "\n",
    "        output = self.fully_connected_3(output)\n",
    "\n",
    "        output = self.fully_connected_4(output)\n",
    "\n",
    "        output = self.fully_connected_5(output)\n",
    "        output = self.fully_connected_6(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    '''\n",
    "    we are using the Adam optimizer with a learning rate that has been initialized in the __init__() method, \n",
    "    and we then return the optimizer from this method.\n",
    "\n",
    "The configure_optimizers method can return up to six different outputs. \n",
    "As in the preceding example, it can also return a single list/tuple object. With multiple optimizers, \n",
    "it can return two separate lists: one for the optimizers, and a second consisting of the learning-rate scheduler.\n",
    "    \n",
    "    '''\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        params = self.parameters()\n",
    "\n",
    "        optimizer = optim.Adam(params=params, lr = self.learning_rate)\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        inputs, targets = batch\n",
    "\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        train_accuracy = accuracy(outputs, targets)\n",
    "\n",
    "        loss = self.loss(outputs, targets)\n",
    "\n",
    "        self.log('train_accuracy', train_accuracy, prog_bar=True)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return {\"loss\":loss, \"train_accuracy\": train_accuracy }\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        inputs, targets = batch\n",
    "\n",
    "        outputs = self.forward(inputs)\n",
    "\n",
    "        #test_accuracy = self.binary_accuracy(outputs,targets)\n",
    "        test_accuracy = accuracy(outputs,targets)\n",
    "\n",
    "        loss = self.loss(outputs, targets)\n",
    "\n",
    "        self.log('test_accuracy', test_accuracy)\n",
    "\n",
    "        return {\"test_loss\":loss, \"test_accuracy\": test_accuracy }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5209b",
   "metadata": {},
   "source": [
    "one of the life cycle methods that helps us to train our model on the training dataset is training_step. Similarly, if we want to test our model on the test dataset, we have a life cycle method called test_step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67cfac",
   "metadata": {},
   "source": [
    "### Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f47fb",
   "metadata": {},
   "source": [
    "One of the key features of the PyTorch Lightning framework is the simplicity with which we can train the model. The trainer class comes in handy for doing training along with easy-to-use options such as picking the hardware (cpu/gpu/tpu), controlling training epochs, and all other nice things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a860c4a",
   "metadata": {},
   "source": [
    "In PyTorch Lightning, to train the model, we first initialize the trainer class and then invoke the fit method to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8c97c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "   | Name              | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0  | conv_layer1       | Conv2d           | 84    \n",
      "1  | relu1             | ReLU             | 0     \n",
      "2  | pool              | MaxPool2d        | 0     \n",
      "3  | conv_layer2       | Conv2d           | 168   \n",
      "4  | relu2             | ReLU             | 0     \n",
      "5  | fully_connected_1 | Linear           | 1.5 M \n",
      "6  | fully_connected_2 | Linear           | 500 K \n",
      "7  | fully_connected_3 | Linear           | 125 K \n",
      "8  | fully_connected_4 | Linear           | 30.1 K\n",
      "9  | fully_connected_5 | Linear           | 7.3 K \n",
      "10 | fully_connected_6 | Linear           | 122   \n",
      "11 | loss              | CrossEntropyLoss | 0     \n",
      "--------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.802     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8599d40561c34dc99d64c0cc01ae01d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNNImageClassifier()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=5, progress_bar_refresh_rate=50)\n",
    "\n",
    "trainer.fit(model, train_dataloader=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682c29c",
   "metadata": {},
   "source": [
    "### Evaluating the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842dcc3",
   "metadata": {},
   "source": [
    "To calculate the accuracy of the model, we need to pass test data into our test data loader and check the accuracy on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21b47c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/ju9yt3r/FOLDER_PYTORCH/TEST/lightning_logs/version_4/checkpoints/epoch=4-step=154.ckpt\n",
      "Loaded model weights from checkpoint at /home/ju9yt3r/FOLDER_PYTORCH/TEST/lightning_logs/version_4/checkpoints/epoch=4-step=154.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b07faa72e141f19859baf79e7e90c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': 0.7375565767288208}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_accuracy': 0.7375565767288208}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(test_dataloaders=test_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1f3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c222d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
